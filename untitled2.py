# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Q31XRdPnarTcF1fRZfTyIZnYqRdNw6b
"""

import streamlit as st
import pandas as pd
import re
import string
import os
import sys

# Try to import required packages with error handling
try:
    import joblib
except ImportError:
    st.error("‚ùå joblib is not installed. Please install it using: pip install joblib")
    st.stop()

try:
    import nltk
    import ssl
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    NLTK_AVAILABLE = True
except ImportError:
    st.warning("‚ö†Ô∏è NLTK is not installed. Using basic text processing.")
    NLTK_AVAILABLE = False

# Set up NLTK only if available
if NLTK_AVAILABLE:
    # Set NLTK data path to a writable directory
    nltk_data_path = os.path.join("/tmp", "nltk_data")
    os.makedirs(nltk_data_path, exist_ok=True)
    nltk.data.path.append(nltk_data_path)

    # Handle SSL certificates for NLTK downloads
    try:
        _create_unverified_https_context = ssl._create_unverified_https_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context

    @st.cache_resource
    def download_nltk_data():
        """Download NLTK data packages with caching and version compatibility"""
        nltk_packages = [
            ('punkt_tab', 'punkt'),  # Try punkt_tab first, fallback to punkt
            ('stopwords', None),
            ('wordnet', None),
            ('omw-1.4', None)  # Additional wordnet data
        ]

        success = True
        for primary, fallback in nltk_packages:
            try:
                nltk.download(primary, download_dir=nltk_data_path, quiet=True)
            except Exception as e1:
                if fallback:
                    try:
                        nltk.download(fallback, download_dir=nltk_data_path, quiet=True)
                    except Exception as e2:
                        st.warning(f"Could not download {primary} or {fallback}: {e2}")
                        success = False
                else:
                    try:
                        nltk.download(primary, download_dir=nltk_data_path, quiet=True)
                    except:
                        st.warning(f"Could not download {primary}")
                        success = False

        return success

    # Download NLTK data
    if not download_nltk_data():
        st.warning("Some NLTK data may be missing, but trying to continue...")

    # Initialize NLTK components with error handling
    @st.cache_resource
    def init_nltk_components():
        """Initialize NLTK components with error handling"""
        try:
            stop_words = set(stopwords.words('english'))
            lemmatizer = WordNetLemmatizer()
            return stop_words, lemmatizer, True
        except Exception as e:
            st.error(f"Error initializing NLTK components: {e}")
            return set(), None, False

    stop_words, lemmatizer, nltk_ready = init_nltk_components()
else:
    stop_words, lemmatizer, nltk_ready = set(), None, False

def clean_text(text):
    """Clean and preprocess text with better error handling"""
    try:
        if not nltk_ready or not NLTK_AVAILABLE:
            # Fallback: basic cleaning without NLTK
            text = str(text).lower()
            text = text.translate(str.maketrans('', '', string.punctuation))
            words = text.split()
            basic_stopwords = {
                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of',
                'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had',
                'do', 'does', 'did', 'will', 'would', 'could', 'should', 'this', 'that', 'these',
                'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us',
                'them', 'my', 'your', 'his', 'its', 'our', 'their'
            }
            cleaned_words = [word for word in words if word not in basic_stopwords and len(word) > 1]
            return ' '.join(cleaned_words)

        # Regular NLTK processing
        text = str(text).lower()
        text = text.translate(str.maketrans('', '', string.punctuation))

        # Try tokenization with error handling
        try:
            tokens = nltk.word_tokenize(text)
        except Exception:
            # Fallback to simple split if tokenizer fails
            tokens = text.split()

        # Remove stopwords and lemmatize
        if lemmatizer:
            cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1]
        else:
            cleaned_tokens = [word for word in tokens if word not in stop_words and len(word) > 1]

        return ' '.join(cleaned_tokens)

    except Exception as e:
        st.error(f"Error cleaning text: {e}")
        # Return basic cleaned version as fallback
        text = str(text).lower()
        text = ''.join(c for c in text if c.isalnum() or c.isspace())
        return text

@st.cache_resource
def load_models():
    """Load the trained model and vectorizer with caching"""
    try:
        # Try different possible paths for model files
        model_paths = [
            'saved_models/best_baseline_model.pkl',
            'best_baseline_model.pkl',
            './saved_models/best_baseline_model.pkl',
            './best_baseline_model.pkl'
        ]

        vectorizer_paths = [
            'saved_models/tfidf_vectorizer.pkl',
            'tfidf_vectorizer.pkl',
            './saved_models/tfidf_vectorizer.pkl',
            './tfidf_vectorizer.pkl'
        ]

        model_file = None
        vectorizer_file = None

        # Find model file
        for path in model_paths:
            if os.path.exists(path):
                model_file = path
                break

        # Find vectorizer file
        for path in vectorizer_paths:
            if os.path.exists(path):
                vectorizer_file = path
                break

        if model_file is None:
            st.error("‚ùå Model file 'best_baseline_model.pkl' not found in any expected location.")
            st.info("Please ensure the model file is in one of these locations:")
            for path in model_paths:
                st.text(f"  ‚Ä¢ {path}")
            return None, None

        if vectorizer_file is None:
            st.error("‚ùå Vectorizer file 'tfidf_vectorizer.pkl' not found in any expected location.")
            st.info("Please ensure the vectorizer file is in one of these locations:")
            for path in vectorizer_paths:
                st.text(f"  ‚Ä¢ {path}")
            return None, None

        # Load models
        baseline_model = joblib.load(model_file)
        tfidf_vectorizer = joblib.load(vectorizer_file)

        st.success(f"‚úÖ Models loaded successfully from {os.path.dirname(model_file) or 'current directory'}")
        return baseline_model, tfidf_vectorizer

    except Exception as e:
        st.error(f"‚ùå Error loading models: {e}")
        return None, None

# Streamlit app configuration
st.set_page_config(
    page_title="BBC News Classifier",
    page_icon="üì∞",
    layout="wide"
)

# App header
st.title("üì∞ BBC News Article Classifier")
st.write("Enter a news article to classify its category using machine learning.")

# Create a requirements.txt info section
with st.expander("üìã Installation Requirements"):
    st.write("""
    If you're running this locally, make sure you have these packages installed:
       streamlit
    joblib
    pandas
    nltk
    scikit-learn

    Install them with:
       pip install streamlit joblib pandas nltk scikit-learn
        """)

# Load models
with st.spinner("Loading models..."):
    baseline_model, tfidf_vectorizer = load_models()

if baseline_model is None or tfidf_vectorizer is None:
    st.error("‚ùå Failed to load models. Please check the error messages above.")
    st.info("Make sure your model files are in the correct location and try refreshing the page.")
    st.stop()

# Define the categories (make sure this matches the order used during training)
categories = ['business', 'entertainment', 'politics', 'sport', 'tech']

# Show processing status
if not nltk_ready and NLTK_AVAILABLE:
    st.warning("‚ö†Ô∏è Using basic text processing (NLTK components not fully loaded)")
elif not NLTK_AVAILABLE:
    st.info("‚ÑπÔ∏è Using basic text processing (NLTK not installed)")
else:
    st.success("‚úÖ All text processing components loaded successfully")

# Main interface
col1, col2 = st.columns([2, 1])

with col1:
    # Text input area
    article_text = st.text_area(
        "Enter Article Text",
        height=300,
        placeholder="Paste your news article text here...",
        help="Enter the full text of a news article for classification"
    )

with col2:
    st.subheader("üìä Categories")
    for i, category in enumerate(categories):
        st.write(f"{i+1}. {category.capitalize()}")

    st.subheader("‚ÑπÔ∏è About")
    st.write("""
    This classifier uses machine learning to categorize BBC news articles into one of five categories:
    - Business: Financial and economic news
    - Entertainment: Celebrity and entertainment news
    - Politics: Political and government news
    - Sport: Sports and athletics news
    - Tech: Technology and science news
    """)

# Classification button and results
if st.button("üîç Classify Article", type="primary"):
    if article_text.strip():
        try:
            with st.spinner("Processing article..."):
                # Clean the input text
                cleaned_article = clean_text(article_text)

                if not cleaned_article.strip():
                    st.warning("‚ö†Ô∏è The text appears to be empty after cleaning. Please try with different content.")
                    st.stop()

                # Vectorize the cleaned text
                article_tfidf = tfidf_vectorizer.transform([cleaned_article])

                # Get prediction probabilities
                probabilities = baseline_model.predict_proba(article_tfidf)[0]

                # Get the predicted category index and confidence
                predicted_category_index = baseline_model.predict(article_tfidf)[0]
                confidence = probabilities[predicted_category_index]

                # Get the predicted category name
                predicted_category = categories[predicted_category_index]

            # Display results
            st.success("‚úÖ Classification completed!")

            result_col1, result_col2 = st.columns(2)

            with result_col1:
                st.subheader("üéØ Prediction")
                st.metric("Category", predicted_category.capitalize(), help="The predicted category")
                st.metric("Confidence", f"{confidence:.1%}", help="How confident the model is")

                # Confidence interpretation
                if confidence >= 0.8:
                    st.success("üü¢ High confidence prediction")
                elif confidence >= 0.6:
                    st.warning("üü° Moderate confidence prediction")
                else:
                    st.error("üî¥ Low confidence prediction")

            with result_col2:
                st.subheader("üìä All Probabilities")
                prob_df = pd.DataFrame({
                    'Category': [cat.capitalize() for cat in categories],
                    'Probability': probabilities,
                    'Percentage': [f"{prob:.1%}" for prob in probabilities]
                })
                prob_df = prob_df.sort_values(by='Probability', ascending=False)

                # Display as a bar chart
                st.bar_chart(prob_df.set_index('Category')['Probability'])

                # Display as a table
                st.dataframe(
                    prob_df[['Category', 'Percentage']],
                    use_container_width=True,
                    hide_index=True
                )

        except Exception as e:
            st.error(f"‚ùå An error occurred during classification: {e}")
            st.error("Please try again with different text or check your model files.")

            # Debug information
            with st.expander("üîß Debug Information"):
                st.write(f"Error type: {type(e).__name__}")
                st.write(f"Error message: {str(e)}")

    else:
        st.warning("‚ö†Ô∏è Please enter some text to classify.")

# Example text section
with st.expander("üìù Try with example articles"):
    examples = {
        "Technology": """
        Apple Inc. announced its latest iPhone model today, featuring an advanced AI processor
        and improved camera technology. The new device includes 5G connectivity and extended
        battery life. Tech analysts expect strong sales in the upcoming quarter as consumers
        upgrade their smartphones. The stock price rose 3% following the announcement.
        """,
        "Sports": """
        Manchester United secured a decisive 3-1 victory over their rivals in yesterday's
        Premier League match. The team's striker scored two goals in the second half,
        bringing his season total to 15 goals. The win moves United up to fourth place
        in the league table with just six games remaining in the season.
        """,
        "Politics": """
        The Prime Minister announced new policy measures aimed at addressing climate change
        during today's parliamentary session. The proposed legislation includes carbon tax
        reforms and increased funding for renewable energy projects. Opposition parties
        have criticized the timeline as too ambitious, calling for more detailed implementation plans.
        """,
        "Business": """
        Major retail chains reported strong quarterly earnings despite economic uncertainty.
        Consumer spending remained robust across most sectors, with online sales showing
        particularly impressive growth. Market analysts attribute the positive results to
        effective cost management and strategic inventory planning during supply chain disruptions.
        """,
        "Entertainment": """
        The annual film awards ceremony took place last night in Los Angeles, celebrating
        outstanding achievements in cinema. The best picture award went to an independent
        drama that tackles social issues. Several surprise wins in acting categories made
        headlines, while the ceremony's host received praise for their comedic performance.
        """
    }

    selected_example = st.selectbox("Choose an example:", list(examples.keys()))

    if selected_example:
        st.text_area("Example text:", examples[selected_example], height=150, disabled=True)
        if st.button(f"üîç Classify this {selected_example} example"):
            # Set the article text and trigger classification
            article_text = examples[selected_example]
            st.rerun()

# Footer
st.markdown("---")
st.markdown("*Built with Streamlit and scikit-learn*")

